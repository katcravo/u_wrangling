{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "filename = 'osm/sample.osm'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "    osm_file = open(filename, \"r\")\n",
    "    print ('opened_file')\n",
    "    tagsdict = defaultdict (lambda: 0)\n",
    "    #for event, elem in ET.iterparse(osm_file, events=('start', )):\n",
    "    #    tagsdict[elem.tag]=tagsdict[elem.tag]+1\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end':\n",
    "            tagsdict[elem.tag]=tagsdict[elem.tag]+1\n",
    "            root.clear()    \n",
    " \n",
    "    return dict(tagsdict)\n",
    "\n",
    "tags = count_tags(filename)\n",
    "pprint.pprint(tags)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Get a list of all the tags and their count\n",
    "\"\"\"\n",
    "\n",
    "def count_tags(filename):\n",
    "    osm_file = open(filename, \"r\")\n",
    "    tagsdict = defaultdict (lambda: 0)\n",
    "    for event, elem in ET.iterparse(osm_file, events=('start', )):\n",
    "        tagsdict[elem.tag]=tagsdict[elem.tag]+1\n",
    "    return dict(tagsdict)\n",
    "\n",
    "tags = count_tags(filename)\n",
    "pprint.pprint(tags)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "check that the tag keys are valid chars\n",
    "\"\"\"\n",
    "import re\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "\n",
    "\n",
    "osm_file = open(filename, \"r\")\n",
    "context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "_, root = next(context)\n",
    "for event, element in context:\n",
    "    if event == 'end':\n",
    "        if element.tag == \"tag\":\n",
    "            key = element.attrib['k']\n",
    "            whichCase = \"other\"\n",
    "            if (lower.match(key)): whichCase = \"lower\"\n",
    "            elif (lower_colon.match(key)): whichCase = \"lower_colon\"\n",
    "            elif (problemchars.search(key)): whichCase = \"problemchars\"\n",
    "            #print key, whichCase\n",
    "            keys [whichCase] = keys [whichCase] + 1\n",
    "    root.clear()\n",
    "    \n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "get the unique users that have modified the map\n",
    "\"\"\"\n",
    "\n",
    "def get_attrib(element, key):\n",
    "    if ( key in element.attrib ):\n",
    "        return element.attrib[key]\n",
    "    return None\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        user  = get_attrib(element, 'user')\n",
    "        if user != None: users.add(user)\n",
    "    return users\n",
    "\n",
    "users = process_map(filename)\n",
    "pprint.pprint(users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Audit Street Types\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "first_word_re = re.compile(r'^\\w+', re.IGNORECASE)\n",
    "old_expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Terrace\", \"Loop\", \"Highway\", \"Course\", \"Circle\", \"Way\", \"Crescent\", \"Walk\"]\n",
    "expected_first = [\"Avenue\"]\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\" : \"Avenue\",\n",
    "            \"Rd.\" : \"Road\",\n",
    "            \"Rd\" : \"Road\"\n",
    "            }\n",
    "extra_suffix = [\"North\", \"East\", \"West\", \"South\"]\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            #print 'Found', street_type\n",
    "            if street_type_first(street_name): return\n",
    "            else: street_types[street_type].add(street_name)\n",
    "            \n",
    "def street_type_first(street_name):\n",
    "    #print 'Check first', street_name\n",
    "    m = first_word_re.search(street_name)\n",
    "    if m:\n",
    "        if m.group() in expected_first:\n",
    "            #print 'Found Street Type first for', street_name\n",
    "            return True\n",
    "    return False\n",
    "            \n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit_street_types_in_file(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    #print tag.attrib['k'], tag.attrib['v']\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        \n",
    "        #print \"Checking \", name\n",
    "        if street_type in expected: pass\n",
    "        elif street_type in mapping: \n",
    "            print \"Fixing name \", name\n",
    "            name = street_type_re.sub(mapping[street_type], name)\n",
    "            #print \"Fixed name \", name\n",
    "        else:\n",
    "            processed_suffix = process_suffix(name, mapping)\n",
    "            if processed_suffix != None: return processed_suffix\n",
    "        \n",
    "        return name     \n",
    "\n",
    "def process_suffix (name, mapping):\n",
    "    split_name = name.split(' ')\n",
    "    if len(split_name) > 2 and split_name[-1] in extra_suffix:\n",
    "        print \"Has a valid suffix \", name\n",
    "        return update_name(name.rsplit(' ', 1)[0], mapping) + ' ' + split_name[-1] \n",
    "    return None\n",
    "\n",
    "unexpected_st_types = audit_street_types_in_file(filename)\n",
    "pprint.pprint(dict(unexpected_st_types))\n",
    "\n",
    "for st_type, ways in unexpected_st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "        print name, \"=>\", better_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "replace_tags = {'addr':'address', 'type': 'type_as_specified'}\n",
    "has_value_and_children = ['hgv', 'name', 'building', 'railway', 'lanes', 'maxspeed', 'source']\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_attributes(element):\n",
    "    attributes = {}\n",
    "    created = {}\n",
    "    #address = {}\n",
    "    pos = [None,None]\n",
    "\n",
    "    for attribute in element.attrib:\n",
    "        value = element.attrib[attribute]\n",
    "        #print attribute\n",
    "\n",
    "        if attribute in CREATED:\n",
    "            created[attribute] = value\n",
    "\n",
    "        elif attribute == 'lat': \n",
    "            try:\n",
    "                pos [0] = float(value)\n",
    "            except ValueError:\n",
    "                print \"Not a float\"                \n",
    "        elif attribute == 'lon':\n",
    "            try:\n",
    "                pos [1] = float(value)\n",
    "            except ValueError:\n",
    "                print \"Not a float\"\n",
    "\n",
    "        else: attributes[attribute] = value    \n",
    "\n",
    "    attributes['created'] = created\n",
    "    if None not in pos: attributes['pos'] = pos\n",
    "        \n",
    "    return attributes    \n",
    "\n",
    "def process_refs(element):\n",
    "    node_refs = []\n",
    "    for ref in element.iter(\"nd\"):\n",
    "        node_refs.append(ref.attrib['ref'])\n",
    "    if len(node_refs) > 0 : return {\"node_refs\":node_refs}\n",
    "    return {}\n",
    "\n",
    "def process_tags(element):\n",
    "    tags = {}\n",
    "    for tag in element.iter(\"tag\"):\n",
    "        process_tag(tag.attrib['k'],tag.attrib['v'], tags)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_tag (name, value, tags):\n",
    "    #tags[name]=value\n",
    "    #if (name.startswith('hgv')): print name, value\n",
    "    try:\n",
    "        if (name!= None and len(name)>0 and name.strip()!=':' and \n",
    "            name.strip()!='' and not problemchars.search(name)):\n",
    "            \n",
    "            levels = name.split(':')\n",
    "            \n",
    "            #extract the top level key and convert it as needed\n",
    "            top = levels[0]\n",
    "            top = replace_tags.get(top,top)\n",
    "            \n",
    "            #assign value if it's a top level attribute\n",
    "            if len(levels) == 1: \n",
    "                if top not in tags: tags[top] = value\n",
    "                else: \n",
    "                    print top, value, ' other value already found as ', tags[top]\n",
    "                \n",
    "            #add to address dict if there are two levels\n",
    "            elif top=='address' and len(levels) == 2:\n",
    "                if 'address' not in tags: tags['address']={}                    \n",
    "                if not isinstance(tags['address'],dict): \n",
    "                    print 'address:', name, value, ' overwriting simple value already found as ', tags['address']\n",
    "                    tags['address']={}\n",
    "                process_tag (levels[1],value,tags['address'])\n",
    "                \n",
    "            #add to a dict if there are multiple levels\n",
    "            elif top!='address' and len(levels) > 1:\n",
    "                if top in has_value_and_children: top = top + \"_data\"\n",
    "                elif top in tags and not isinstance(tags[top],dict): \n",
    "                    #a root value was already found for this tag.\n",
    "                    print name, value, ' unexpected - simple value already found as ', top, tags[top]\n",
    "                    return\n",
    "                if top not in tags: tags[top]={}                    \n",
    "                process_tag (name.split(':', 1)[1], value, tags[top])\n",
    "                \n",
    "    except:\n",
    "        print 'Exception', name, value\n",
    "        pprint.pprint(tags)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_tags = ['street']\n",
    "def fix_street(tags):\n",
    "    if 'address' in tags:\n",
    "        addr = tags['address']\n",
    "        for key in addr:\n",
    "            if key in street_tags:\n",
    "                #print 'checking', addr[key]\n",
    "                addr[key] = update_name(addr[key], mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node['type']=element.tag        \n",
    "        refs = process_refs(element)\n",
    "        node.update(refs)\n",
    "        attrs = process_attributes(element)    \n",
    "        node.update(attrs)\n",
    "        tags = process_tags(element)\n",
    "        fix_street(tags)\n",
    "        node.update(tags)\n",
    "              \n",
    "        #pprint.pprint( node )\n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "x = process_map(filename, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_tags(node,parent,tags):\n",
    "    for key in node:\n",
    "        #print key, node[key]\n",
    "        if isinstance (node[key],dict): \n",
    "            #print \"found dict for \", key\n",
    "            extract_tags(node[key] , parent + key + \".\", tags)\n",
    "        else:\n",
    "            if key not in tags: tags[key] = set()\n",
    "            tags[key].add( parent+key )\n",
    "\n",
    "def check_for_unique_tags(jsonFile):\n",
    "    f = open(jsonFile)\n",
    "    tags = defaultdict(set)\n",
    "    for line in iter(f):\n",
    "        #print line\n",
    "        node = json.loads(line)\n",
    "        extract_tags(node, '', tags)\n",
    "    f.close()\n",
    "    #pprint.pprint (dict(tags))\n",
    "    multis = {k: v for k, v in (dict(tags)).items() if len(tags[k])>1}\n",
    "    pprint.pprint (multis)\n",
    "    \n",
    "check_for_unique_tags(filename + '.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "def isint(value):\n",
    "  try:\n",
    "    int(value)\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "def extract_data_types(node,parent,tags):\n",
    "    for key in node:\n",
    "        location = parent + key\n",
    "        #print key, node[key]\n",
    "        if isinstance (node[key],dict): \n",
    "            #print \"found dict for \", key\n",
    "            extract_data_types(node[key] , location + \".\", tags)\n",
    "        else:\n",
    "            if location not in tags: tags[location] = set()\n",
    "            tags[location].add( get_data_type(node[key]) )\n",
    "\n",
    "def get_data_type(value):\n",
    "    if value == \"\" or value == \"NULL\":\n",
    "        return type(None)\n",
    "    elif type(value) is list:\n",
    "        return type([])\n",
    "    elif isint(value):\n",
    "        return type(int())\n",
    "    elif isfloat(value):\n",
    "        return type(float())\n",
    "    else:\n",
    "        return type(str())\n",
    "\n",
    "def audit_data_types_in_file(jsonFile):\n",
    "    f = open(jsonFile)\n",
    "    tags = defaultdict(set)\n",
    "    for line in iter(f):\n",
    "        #print line\n",
    "        node = json.loads(line)\n",
    "        extract_data_types(node, '', tags)\n",
    "    f.close()\n",
    "    return dict(tags)\n",
    "    \n",
    "data_types = audit_data_types_in_file(filename + '.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pprint.pprint(data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pick_data_type(data_types):\n",
    "    pick_type = {}\n",
    "    for key in data_types:\n",
    "        if  type([]) in data_types[key]:\n",
    "            pick_type [key] = type([])\n",
    "        elif  type(str()) in data_types[key]:\n",
    "            pick_type [key] = type(str())\n",
    "        elif  type(float()) in data_types[key]:\n",
    "            pick_type [key] = type(float())\n",
    "        elif  type(int()) in data_types[key]:\n",
    "            pick_type [key] = type(int())\n",
    "        else:\n",
    "            pick_type [key] =  type(str())\n",
    "    return pick_type\n",
    "\n",
    "data_types = audit_data_types_in_file(filename + '.json')\n",
    "save_as_data_type = pick_data_type(data_types)\n",
    "#pprint.pprint (save_as_data_type)\n",
    "\n",
    "ints = [ key for (key, value) in save_as_data_type.items() if value == type(int()) and not 'zip' in key]\n",
    "print ints\n",
    "#not_ints = [ key for key in ints if key == type(int()) ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "C:\\mongodb24\\mongodb-win32-x86_64-2008plus-2.4.3\\bin>mongoimport -d osm -c sample --file C:\\Users\\Kathleen\\Documents\\udacity\\wrangling\\osm\\sample.osm.json\n",
    "connected to: 127.0.0.1\n",
    "Fri Nov 04 05:35:01.961 check 9 10952\n",
    "Fri Nov 04 05:35:01.963 imported 10952 objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_db():\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    # 'examples' here is the database name. It will be created if it does not exist.\n",
    "    db = client.osm\n",
    "    return db\n",
    "\n",
    "db = get_db()\n",
    "db.sample.find_one()\n",
    "\n",
    "#db.test.update({field:\"string\"}, {$set:{field:23}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.sample.find_one({\"address.postcode\" : { \"$exists\" : True } })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_zip_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    \n",
    "    #match brasilia time zone and number of tweets > 100\n",
    "    #project to _id, followers, screen_name, tweets\n",
    "    #sort by followers descenging\n",
    "    \n",
    "    pipeline = [ \n",
    "        { \"$match\": { \"address.postcode\" : { \"$exists\" : True } } },\n",
    "        { \"$group\" : { \"_id\": \"$address.postcode\", \"count\" : {\"$sum\" : 1 }}},\n",
    "        { \"$sort\": {\"_id\":1 }} ]\n",
    "        #{ \"$sort\": {\"address.postcode\":-1 }},\n",
    "        #{ \"$limit\" : 1 } ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.sample.aggregate(pipeline)]\n",
    "\n",
    "\n",
    "db = get_db()\n",
    "pipeline = make_zip_pipeline()\n",
    "result = aggregate(db, pipeline)\n",
    "#import pprint\n",
    "#pprint.pprint(result)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = db.sample.find({\"address.postcode\" : '08816'})\n",
    "for rec in res:\n",
    "    pprint.pprint (rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(db.sample.find_one({\"address.postcode\" : '08854'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrong_length = [ rec['_id'] for rec in result if len(rec['_id']) != 5]\n",
    "\n",
    "##for rec in result:\n",
    "##    zip = rec['_id']\n",
    "##    wrong_length.append(zip)\n",
    "\n",
    "print wrong_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(db.sample.find_one({\"address.postcode\" : '115422'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = db.sample.find({\"pos\" : None})\n",
    "print res.count()\n",
    "res = db.sample.find()\n",
    "print res.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_distinct_with_count_pipeline( by_field ):    \n",
    "    pipeline = [ \n",
    "        { \"$match\": { by_field : { \"$exists\" : True } } },\n",
    "        { \"$group\" : { \"_id\": \"$\" + by_field , \"count\" : {\"$sum\" : 1 }}},\n",
    "        { \"$sort\": {\"count\":-1 }} ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.sample.aggregate(pipeline)]\n",
    "\n",
    "def distinct_with_count(field_name):\n",
    "    db = get_db()\n",
    "    pipeline = make_distinct_with_count_pipeline(field_name)\n",
    "    result = aggregate(db, pipeline)\n",
    "    return result\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(result)\n",
    "result = distinct_with_count('address.state')\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_countdistinct_pipeline( by_field ):    \n",
    "    pipeline = [ \n",
    "        { \"$match\": { by_field : { \"$exists\" : True } } },\n",
    "        { \"$group\" : { \"_id\": \"$\" + by_field }},\n",
    "        { \"$group\": { \"_id\": by_field, \"count\" : {\"$sum\" : 1 } } } ]\n",
    "    return pipeline\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in db.sample.aggregate(pipeline)]\n",
    "\n",
    "def count_distinct(field_name):\n",
    "    db = get_db()\n",
    "    pipeline = make_countdistinct_pipeline(field_name)\n",
    "    result = aggregate(db, pipeline)\n",
    "    return result[0]\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(result)\n",
    "result = count_distinct('address.state')\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = db.sample.find({\"address.state\" : {\"$exists\": True , \"$ne\": 'NY'}})\n",
    "#print res\n",
    "for rec in res:\n",
    "    pprint.pprint (rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = get_db()\n",
    "pipeline = make_countdistinct_pipeline('address.city')\n",
    "result = aggregate(db, pipeline)\n",
    "#import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = db.sample.find({\"$and\":[{\"tiger.name_base\" : {\"$exists\": True }}, \n",
    "                              {\"address.street\": {\"$exists\": True}} ] })\n",
    "print res.count()\n",
    "#for rec in res:\n",
    "#    pprint.pprint (rec)\n",
    "\n",
    "res = db.sample.find_one({\"tiger.name_base\": {\"$exists\": True}})\n",
    "pprint.pprint(res)\n",
    "\n",
    "matchy = db.sample.find({\"address.street\": {\"$regex\" : \"82\"} })\n",
    "print matchy.count()\n",
    "for rec in matchy: pprint.pprint (rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_check_ways ():\n",
    "    no_match = {}\n",
    "    maybe_match = {}\n",
    "    res = db.sample.find({\"tiger.name_base\": {\"$exists\": True}})\n",
    "    print \"tiger #\", res.count()\n",
    "    for rec in res:\n",
    "        name = rec[\"tiger\"][\"name_base\"]\n",
    "        matchy = db.sample.find({\"address.street\": {\"$regex\" : name} })\n",
    "        if matchy.count()>0: \n",
    "            if name not in maybe_match: \n",
    "                maybe_match[name] = [a for a in matchy]\n",
    "            maybe_match[name].insert(0,rec)\n",
    "        else: no_match[name] = rec\n",
    "            \n",
    "    print \"maybe match #\", len(maybe_match)\n",
    "    print \"no match #\", len(no_match)\n",
    "    return maybe_match, no_match\n",
    "\n",
    "(maybe_match, no_match) = cross_check_ways()\n",
    "#pprint.pprint(maybe_match[maybe_match.keys()[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for street in maybe_match.keys() [:2]:\n",
    "    print '*****  ' , street, '  *****'\n",
    "    for rec in maybe_match[street]:\n",
    "        try: \n",
    "            x = rec['address']['street'] + ' ' + '(address)'\n",
    "            print x\n",
    "        except: pass\n",
    "        try:\n",
    "            x = rec['tiger']['name_direction_prefix']+ ' ' + rec['tiger']['name_base']+ ' ' +  rec['tiger']['name_type']+ ' ' +  '(tiger)'\n",
    "            print x\n",
    "        except: pass\n",
    "        try:\n",
    "            x = rec['tiger']['name_base']+ ' ' +  rec['tiger']['name_type']+ ' ' +  '(tiger)'\n",
    "            print x\n",
    "        except: pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = db.sample.count()\n",
    "print size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "types = count_distinct(\"type\")\n",
    "pprint.pprint(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_count_distincts(fields):\n",
    "    for field_name in fields:\n",
    "        #pprint.pprint(get_top(field_name))\n",
    "        count_dist = count_distinct(field_name)\n",
    "        print \"Distinct \", field_name, count_dist['_id'], ' - count', count_dist['count']\n",
    "\n",
    "get_count_distincts_fields = ['created.user', 'address.postcode', 'address.city', 'amenity','source']\n",
    "print_count_distincts(get_count_distincts_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_get_top_pipeline( by_field ):    \n",
    "    pipeline = [ \n",
    "        { \"$match\": { by_field : { \"$exists\" : True } } },\n",
    "        { \"$group\" : { \"_id\": \"$\" + by_field , \"count\" : {\"$sum\" : 1 }}},\n",
    "        { \"$sort\": { \"count\" :-1 }},\n",
    "        { \"$limit\" : 1 } ]\n",
    "    return pipeline\n",
    "\n",
    "def get_top(field_name):\n",
    "    db = get_db()\n",
    "    pipeline = make_get_top_pipeline(field_name)\n",
    "    result = aggregate(db, pipeline)\n",
    "    return result[0]\n",
    "\n",
    "def print_tops(fields):\n",
    "    for field_name in fields:\n",
    "        #pprint.pprint(get_top(field_name))\n",
    "        top = get_top(field_name)\n",
    "        print \"Top\", field_name, top['_id'], ' - count', top['count']\n",
    "\n",
    "#import pprint\n",
    "#pprint.pprint(result)\n",
    "get_top_fields = ['created.user', 'address.postcode', 'address.city', 'amenity','source']\n",
    "print_tops(get_top_fields)\n",
    "\n",
    "\n",
    "#([{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":Â­1}}, {\"$limit\":1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinct_with_count(\"cuisine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinct_with_count(\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_distincts = [\"cuisine\", \"source\", \"leisure\", \"office\", \"service\", \"shelter\", \"shop\", \"sport\", \"gnis.feature_type\"]\n",
    "\n",
    "for field_name in get_distincts:\n",
    "    print \"Distinct\", field_name\n",
    "    pprint.pprint (distinct_with_count(field_name))\n",
    "    print \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
